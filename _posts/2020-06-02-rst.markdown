---
layout: post
title:  "Examining the rhetorical capacities of neural language models"
date:   2020-11-15 22:21:59 +00:00
image: /images/rst.png
categories: research
author: "Chuer Pan"
authors: "Zining Zhu, <strong>Chuer Pan</strong>, Mohamed Abdalla, Frank Rudzicz"
venue: "Conference on Empirical Methods in Natural Language Processing (EMNLP) BlackboxNLP Workshop"
paper: https://aclanthology.org/2020.blackboxnlp-1.3/
video: https://www.youtube.com/watch?v=cmDc1HuGKWE
---
We propose a method that quantitatively evaluates the rhetorical capacities of neural language models (LMs) by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs outperform other Transformer LMs such as GPT-2 and XLNet, revealing richer discourse knowledge in their intermediate layer representations.